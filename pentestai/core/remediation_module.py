"""
Stage 2: Remediation Module - Optimal Remediation Planning

Implements the Remediation Module from the PenHeal paper with 4 agents:
1. Estimator - Assigns CVSS severity scores to vulnerabilities
2. Advisor - Generates multiple remediation strategies per vulnerability
3. Evaluator - Scores strategies with counterfactual analysis (value/cost)
4. Optimizer - Selects optimal set using Group Knapsack algorithm

Key Innovation: Group Knapsack optimization ensures maximum effectiveness
within budget constraints while avoiding redundant remediations.

Architecture based on: arXiv:2407.17788v1 (PenHeal Paper)
"""

import logging
import time
from typing import Any, Optional

from pentestai.models.data import (
    RemediationStrategy,
    RemediationResult,
    RemediationType,
    Vulnerability,
    VulnerabilitySeverity,
)
from pentestai.core.config import PentestAIConfig
from pentestai.knowledge.base import get_knowledge_by_category
from pentestai.llm.client import LLMClient, OpenAIClient, MockLLMClient

logger = logging.getLogger(__name__)


class EstimatorAgent:
    """
    Estimator Agent - Assigns CVSS severity scores to vulnerabilities.
    
    For known CVEs: Uses actual CVSS scores
    For unknown vulnerabilities: Estimates severity based on characteristics
    """

    # Default CVSS scores for common vulnerability types
    SEVERITY_ESTIMATES = {
        "SQL Injection": 8.5,
        "Cross-Site Scripting": 6.1,
        "Authentication Bypass": 9.1,
        "Weak Authentication": 7.5,
        "Anonymous Access": 6.5,
        "Security Misconfiguration": 5.3,
        "Directory Traversal": 7.7,
        "Remote Code Execution": 9.8,
        "Privilege Escalation": 8.8,
        "Information Disclosure": 4.3,
        "Backdoor": 10.0,
        "Buffer Overflow": 9.3,
        "Command Injection": 9.0,
    }

    def __init__(self, config: PentestAIConfig, llm_client: Optional[LLMClient] = None):
        self.config = config
        self.llm = llm_client or MockLLMClient()

    def estimate_severity(self, vulnerability: Vulnerability) -> Vulnerability:
        """
        Estimate or confirm vulnerability severity and CVSS score.
        
        Args:
            vulnerability: Vulnerability to assess
            
        Returns:
            Vulnerability with updated severity and CVSS score
        """
        # If CVE exists and score already set, use it
        if vulnerability.cve_id and vulnerability.cvss_score > 0:
            vulnerability.severity = self._score_to_severity(vulnerability.cvss_score)
            logger.info(f"Using existing CVSS score {vulnerability.cvss_score} for {vulnerability.cve_id}")
            return vulnerability
        
        # Estimate based on vulnerability type/name
        estimated_score = self._estimate_score(vulnerability)
        vulnerability.cvss_score = estimated_score
        vulnerability.severity = self._score_to_severity(estimated_score)
        
        logger.info(f"Estimated CVSS score {estimated_score} for {vulnerability.name}")
        return vulnerability

    def _estimate_score(self, vulnerability: Vulnerability) -> float:
        """Estimate CVSS score based on vulnerability characteristics."""
        name_lower = vulnerability.name.lower()
        
        # Check against known patterns
        for pattern, score in self.SEVERITY_ESTIMATES.items():
            if pattern.lower() in name_lower:
                return score
        
        # Default estimation based on description keywords
        desc_lower = vulnerability.description.lower()
        
        if any(word in desc_lower for word in ["critical", "remote code", "rce", "backdoor"]):
            return 9.5
        elif any(word in desc_lower for word in ["high", "privilege", "bypass", "injection"]):
            return 8.0
        elif any(word in desc_lower for word in ["medium", "disclosure", "xss"]):
            return 6.0
        else:
            return 5.0  # Default medium-low

    def _score_to_severity(self, score: float) -> VulnerabilitySeverity:
        """Convert CVSS score to severity level."""
        if score >= 9.0:
            return VulnerabilitySeverity.CRITICAL
        elif score >= 7.0:
            return VulnerabilitySeverity.HIGH
        elif score >= 4.0:
            return VulnerabilitySeverity.MEDIUM
        elif score > 0.0:
            return VulnerabilitySeverity.LOW
        else:
            return VulnerabilitySeverity.INFO


class AdvisorAgent:
    """
    Advisor Agent - Generates remediation strategies for vulnerabilities.
    
    Creates multiple strategy options per vulnerability using LLM:
    - Patching/updating
    - Reconfiguration
    - Monitoring/detection
    - Isolation/firewall rules
    - Compensating controls
    """

    def __init__(self, config: PentestAIConfig, llm_client: Optional[LLMClient] = None):
        self.config = config
        
        # Initialize LLM client
        if llm_client:
            self.llm = llm_client
        elif config.openai_api_key:
            self.llm = OpenAIClient(
                api_key=config.openai_api_key,
                model=config.advisor_model,
            )
        else:
            logger.warning("No OpenAI API key - using mock LLM client")
            self.llm = MockLLMClient()

    def generate_strategies(self, vulnerabilities: list[Vulnerability]) -> dict[str, list[RemediationStrategy]]:
        """
        Generate remediation strategies for all vulnerabilities.
        
        Args:
            vulnerabilities: List of vulnerabilities to remediate
            
        Returns:
            Dictionary mapping vulnerability ID to list of strategies
        """
        logger.info(f"Generating remediation strategies for {len(vulnerabilities)} vulnerabilities")
        
        strategies_by_vuln = {}
        
        for vuln in vulnerabilities:
            strategies = self._generate_strategies_for_vulnerability(vuln)
            strategies_by_vuln[vuln.id] = strategies
            logger.info(f"Generated {len(strategies)} strategies for {vuln.name}")
        
        return strategies_by_vuln

    def _generate_strategies_for_vulnerability(self, vuln: Vulnerability) -> list[RemediationStrategy]:
        """Generate multiple remediation strategies for a single vulnerability."""
        strategies = []
        
        # Strategy 1: Patch/Update (if available)
        if vuln.cve_id or "backdoor" in vuln.name.lower():
            strategies.append(self._create_patch_strategy(vuln))
        
        # Strategy 2: Service Reconfiguration
        strategies.append(self._create_reconfigure_strategy(vuln))
        
        # Strategy 3: Firewall/Isolation
        if vuln.port:
            strategies.append(self._create_isolation_strategy(vuln))
        
        # Strategy 4: Monitoring/Detection
        strategies.append(self._create_monitoring_strategy(vuln))
        
        # Strategy 5: Compensating Control (for high-severity only)
        if vuln.severity in [VulnerabilitySeverity.CRITICAL, VulnerabilitySeverity.HIGH]:
            strategies.append(self._create_compensating_control_strategy(vuln))
        
        return strategies

    def _create_patch_strategy(self, vuln: Vulnerability) -> RemediationStrategy:
        """Create a patching/update strategy."""
        service_lower = vuln.service.lower()
        
        if "ftp" in service_lower or "vsftpd" in vuln.name.lower():
            description = "Update vsFTPd to latest version to patch backdoor"
            commands = [
                "sudo apt-get update",
                "sudo apt-get install --only-upgrade vsftpd",
                "sudo systemctl restart vsftpd",
            ]
        elif "irc" in service_lower or "unreal" in vuln.name.lower():
            description = "Update UnrealIRCd to latest version"
            commands = [
                "wget https://www.unrealircd.org/downloads/unrealircd-latest.tar.gz",
                "tar xzf unrealircd-latest.tar.gz",
                "cd unrealircd-* && ./Config && make && make install",
            ]
        elif "samba" in service_lower or "smb" in service_lower:
            description = "Update Samba to patched version"
            commands = [
                "sudo apt-get update",
                "sudo apt-get install --only-upgrade samba",
                "sudo systemctl restart smbd",
            ]
        elif "sql" in vuln.name.lower():
            description = "Apply web application security patches"
            commands = [
                "# Update web application framework",
                "# Apply security patches from vendor",
                "# Implement prepared statements in database queries",
            ]
        else:
            description = f"Apply security update for {vuln.service}"
            commands = [f"sudo apt-get update && sudo apt-get upgrade {vuln.service}"]
        
        return RemediationStrategy(
            vulnerability_id=vuln.id,
            type=RemediationType.PATCH,
            description=description,
            commands=commands,
            estimated_time=15,
            risk_level="LOW",
        )

    def _create_reconfigure_strategy(self, vuln: Vulnerability) -> RemediationStrategy:
        """Create a reconfiguration strategy."""
        service_lower = vuln.service.lower()
        
        if "ftp" in service_lower:
            description = "Disable anonymous FTP access and restrict permissions"
            commands = [
                "sudo nano /etc/vsftpd.conf  # Set anonymous_enable=NO",
                "sudo systemctl restart vsftpd",
            ]
        elif "ssh" in service_lower:
            description = "Harden SSH configuration"
            commands = [
                "sudo nano /etc/ssh/sshd_config  # Set PermitRootLogin no",
                "sudo nano /etc/ssh/sshd_config  # Set PasswordAuthentication no",
                "sudo systemctl restart sshd",
            ]
        elif "samba" in service_lower or "smb" in service_lower:
            description = "Configure Samba access controls and disable guest access"
            commands = [
                "sudo nano /etc/samba/smb.conf  # Set 'map to guest = never'",
                "sudo smbpasswd -x guest",
                "sudo systemctl restart smbd",
            ]
        elif "http" in service_lower or "web" in service_lower:
            description = "Enable web application firewall and security headers"
            commands = [
                "sudo apt-get install modsecurity-crs",
                "# Add security headers to web server config",
                "sudo systemctl reload apache2",
            ]
        else:
            description = f"Apply security configuration for {vuln.service}"
            commands = [f"# Review and harden {vuln.service} configuration"]
        
        return RemediationStrategy(
            vulnerability_id=vuln.id,
            type=RemediationType.RECONFIGURE,
            description=description,
            commands=commands,
            estimated_time=20,
            risk_level="LOW",
        )

    def _create_isolation_strategy(self, vuln: Vulnerability) -> RemediationStrategy:
        """Create a firewall/isolation strategy."""
        description = f"Restrict access to port {vuln.port} with firewall rules"
        commands = [
            f"sudo ufw allow from <trusted_network> to any port {vuln.port}",
            f"sudo ufw deny {vuln.port}",
            "sudo ufw reload",
        ]
        
        return RemediationStrategy(
            vulnerability_id=vuln.id,
            type=RemediationType.ISOLATE,
            description=description,
            commands=commands,
            estimated_time=10,
            risk_level="MEDIUM",
        )

    def _create_monitoring_strategy(self, vuln: Vulnerability) -> RemediationStrategy:
        """Create a monitoring/detection strategy."""
        description = f"Implement monitoring and alerting for {vuln.service}"
        commands = [
            f"# Install monitoring tools (e.g., fail2ban for {vuln.service})",
            "sudo apt-get install fail2ban",
            f"# Configure logging for {vuln.service}",
            "# Set up SIEM alerts for exploitation attempts",
        ]
        
        return RemediationStrategy(
            vulnerability_id=vuln.id,
            type=RemediationType.MONITOR,
            description=description,
            commands=commands,
            estimated_time=30,
            risk_level="LOW",
        )

    def _create_compensating_control_strategy(self, vuln: Vulnerability) -> RemediationStrategy:
        """Create a compensating control strategy."""
        description = f"Implement additional security controls for {vuln.service}"
        commands = [
            "# Deploy IDS/IPS to detect exploitation attempts",
            "# Implement network segmentation",
            "# Enable enhanced logging and auditing",
            "# Deploy application-level firewall",
        ]
        
        return RemediationStrategy(
            vulnerability_id=vuln.id,
            type=RemediationType.COMPENSATING_CONTROL,
            description=description,
            commands=commands,
            estimated_time=60,
            risk_level="MEDIUM",
        )


class EvaluatorAgent:
    """
    Evaluator Agent - Scores remediation strategies using counterfactual analysis.
    
    Assigns two scores to each strategy:
    1. Value Score (0-10): Effectiveness - "If applied, how much risk is reduced?"
    2. Cost Score (0-10): Resource cost - time, money, risk of disruption
    
    Uses counterfactual reasoning: "Imagine the strategy is implemented.
    How vulnerable is the system now?"
    """

    def __init__(self, config: PentestAIConfig, llm_client: Optional[LLMClient] = None):
        self.config = config
        self.llm = llm_client or MockLLMClient()

    def evaluate_strategies(
        self,
        strategies_by_vuln: dict[str, list[RemediationStrategy]],
        vulnerabilities: list[Vulnerability]
    ) -> dict[str, list[RemediationStrategy]]:
        """
        Evaluate all strategies with value and cost scores.
        
        Args:
            strategies_by_vuln: Strategies grouped by vulnerability ID
            vulnerabilities: Original vulnerabilities
            
        Returns:
            Evaluated strategies with scores
        """
        logger.info("Evaluating remediation strategies with counterfactual analysis")
        
        # Create vulnerability lookup
        vuln_dict = {v.id: v for v in vulnerabilities}
        
        for vuln_id, strategies in strategies_by_vuln.items():
            vuln = vuln_dict.get(vuln_id)
            if not vuln:
                continue
            
            for strategy in strategies:
                strategy.value_score = self._compute_value_score(strategy, vuln)
                strategy.cost_score = self._compute_cost_score(strategy)
        
        logger.info("Strategy evaluation completed")
        return strategies_by_vuln

    def _compute_value_score(self, strategy: RemediationStrategy, vuln: Vulnerability) -> float:
        """
        Compute effectiveness value score using counterfactual reasoning.
        
        Counterfactual prompt: "If this strategy is applied, how much does
        the system's vulnerability score decrease?"
        """
        base_value = vuln.cvss_score  # Maximum possible value
        
        # Adjust based on strategy type and effectiveness
        if strategy.type == RemediationType.PATCH:
            # Patches fully address the vulnerability
            effectiveness = 1.0
        elif strategy.type == RemediationType.RECONFIGURE:
            # Reconfiguration is highly effective
            effectiveness = 0.9
        elif strategy.type == RemediationType.ISOLATE:
            # Isolation is effective but may have workarounds
            effectiveness = 0.7
        elif strategy.type == RemediationType.COMPENSATING_CONTROL:
            # Compensating controls add layers but don't fix root cause
            effectiveness = 0.6
        elif strategy.type == RemediationType.MONITOR:
            # Monitoring detects but doesn't prevent
            effectiveness = 0.3
        else:
            effectiveness = 0.5
        
        # Adjust for risk level of applying the strategy
        if strategy.risk_level == "HIGH":
            effectiveness *= 0.8  # High-risk strategies are less desirable
        
        value_score = base_value * effectiveness
        
        logger.debug(f"Value score for {strategy.type.value}: {value_score:.2f} "
                    f"(base: {base_value}, effectiveness: {effectiveness})")
        
        return round(value_score, 2)

    def _compute_cost_score(self, strategy: RemediationStrategy) -> float:
        """
        Compute resource cost score.
        
        Lower is better. Based on:
        - Time required
        - Risk of disruption
        - Complexity
        """
        # Base cost from config
        cost_weights = self.config.cost_weights
        
        # Get base cost for strategy type
        type_key = strategy.type.value
        base_cost = cost_weights.get(type_key, 5.0)
        
        # Adjust for risk level
        if strategy.risk_level == "HIGH":
            base_cost += 3.0
        elif strategy.risk_level == "MEDIUM":
            base_cost += 1.0
        
        # Adjust for estimated time (higher time = higher cost)
        if strategy.estimated_time > 60:
            base_cost += 2.0
        elif strategy.estimated_time > 30:
            base_cost += 1.0
        
        # Cap at 10.0
        cost_score = min(base_cost, 10.0)
        
        logger.debug(f"Cost score for {strategy.type.value}: {cost_score:.2f}")
        
        return round(cost_score, 2)


class OptimizerAgent:
    """
    Optimizer Agent - Selects optimal remediation set using Group Knapsack algorithm.
    
    Problem formulation:
    - Items: Remediation strategies (grouped by vulnerability)
    - Constraint: Each vulnerability can have at most one strategy selected
    - Objective: Maximize total value within budget constraint
    
    Algorithm: Dynamic Programming solution to Group Knapsack problem
    Complexity: O(n Ã— B) where n = number of strategies, B = budget
    """

    def __init__(self, config: PentestAIConfig):
        self.config = config

    def optimize(
        self,
        strategies_by_vuln: dict[str, list[RemediationStrategy]],
        budget: float
    ) -> list[RemediationStrategy]:
        """
        Select optimal remediation strategies within budget.
        
        Args:
            strategies_by_vuln: Strategies grouped by vulnerability ID
            budget: Total budget available
            
        Returns:
            Optimal list of selected strategies
        """
        logger.info(f"Optimizing remediation selection with budget: {budget}")
        
        if not self.config.enable_optimization:
            # Fallback: Select best strategy per vulnerability until budget exhausted
            return self._greedy_selection(strategies_by_vuln, budget)
        
        # Apply Group Knapsack algorithm
        selected = self._group_knapsack(strategies_by_vuln, budget)
        
        total_value = sum(s.value_score for s in selected)
        total_cost = sum(s.cost_score for s in selected)
        
        logger.info(f"Optimization complete. Selected {len(selected)} strategies. "
                   f"Total value: {total_value:.2f}, Total cost: {total_cost:.2f}")
        
        return selected

    def _group_knapsack(
        self,
        strategies_by_vuln: dict[str, list[RemediationStrategy]],
        budget: float
    ) -> list[RemediationStrategy]:
        """
        Solve Group Knapsack problem using dynamic programming.
        
        This is the core optimization algorithm from the paper.
        """
        # Convert budget to integer for DP (multiply by 10 for precision)
        budget_int = int(budget * 10)
        
        # Initialize DP table
        # dp[i][b] = (max_value, selected_strategies) for first i groups with budget b
        groups = list(strategies_by_vuln.items())
        n_groups = len(groups)
        
        # DP table: dp[group_idx][budget] = (value, strategy_id)
        dp = [[0.0 for _ in range(budget_int + 1)] for _ in range(n_groups + 1)]
        selected = [[None for _ in range(budget_int + 1)] for _ in range(n_groups + 1)]
        
        # Fill DP table
        for i in range(1, n_groups + 1):
            vuln_id, strategies = groups[i - 1]
            
            for b in range(budget_int + 1):
                # Option 1: Don't select any strategy for this vulnerability
                dp[i][b] = dp[i - 1][b]
                selected[i][b] = selected[i - 1][b]
                
                # Option 2: Select one of the strategies
                for strategy in strategies:
                    cost_int = int(strategy.cost_score * 10)
                    
                    if cost_int <= b:
                        new_value = dp[i - 1][b - cost_int] + strategy.value_score
                        
                        if new_value > dp[i][b]:
                            dp[i][b] = new_value
                            selected[i][b] = (strategy.id, selected[i - 1][b - cost_int])
        
        # Backtrack to find selected strategies
        result_strategies = []
        current = selected[n_groups][budget_int]
        
        # Build strategy ID lookup
        all_strategies = {}
        for strategies in strategies_by_vuln.values():
            for s in strategies:
                all_strategies[s.id] = s
        
        while current is not None:
            strategy_id, prev = current
            if strategy_id:
                result_strategies.append(all_strategies[strategy_id])
            current = prev
        
        return result_strategies

    def _greedy_selection(
        self,
        strategies_by_vuln: dict[str, list[RemediationStrategy]],
        budget: float
    ) -> list[RemediationStrategy]:
        """
        Fallback greedy algorithm: Select best value/cost ratio per vulnerability.
        """
        selected = []
        remaining_budget = budget
        
        for vuln_id, strategies in strategies_by_vuln.items():
            # Find best value/cost ratio
            best_strategy = None
            best_ratio = 0.0
            
            for strategy in strategies:
                if strategy.cost_score <= remaining_budget:
                    ratio = strategy.value_score / max(strategy.cost_score, 0.1)
                    if ratio > best_ratio:
                        best_ratio = ratio
                        best_strategy = strategy
            
            if best_strategy:
                selected.append(best_strategy)
                remaining_budget -= best_strategy.cost_score
        
        return selected


class RemediationModule:
    """
    Complete Remediation Module (Stage 2) coordinating all agents.
    
    Workflow:
    1. Estimator assigns/confirms CVSS scores
    2. Advisor generates multiple strategies per vulnerability
    3. Evaluator scores strategies (value and cost)
    4. Optimizer selects optimal set using Group Knapsack
    """

    def __init__(self, config: PentestAIConfig, llm_clients: Optional[dict] = None):
        self.config = config
        
        # Initialize agents with LLM clients
        if llm_clients:
            self.estimator = EstimatorAgent(config, llm_clients.get("evaluator"))
            self.advisor = AdvisorAgent(config, llm_clients.get("advisor"))
            self.evaluator = EvaluatorAgent(config, llm_clients.get("evaluator"))
            self.optimizer = OptimizerAgent(config)
        else:
            self.estimator = EstimatorAgent(config)
            self.advisor = AdvisorAgent(config)
            self.evaluator = EvaluatorAgent(config)
            self.optimizer = OptimizerAgent(config)

    def run_remediation(self, vulnerabilities: list[Vulnerability]) -> RemediationResult:
        """
        Generate optimal remediation plan for vulnerabilities.
        
        Args:
            vulnerabilities: List of discovered vulnerabilities
            
        Returns:
            Complete remediation results with selected strategies
        """
        logger.info(f"Starting remediation planning for {len(vulnerabilities)} vulnerabilities")
        start_time = time.time()
        
        # Step 1: Estimate severity scores
        for vuln in vulnerabilities:
            self.estimator.estimate_severity(vuln)
        
        # Step 2: Generate remediation strategies
        strategies_by_vuln = self.advisor.generate_strategies(vulnerabilities)
        
        # Flatten for statistics
        all_strategies = []
        for strategies in strategies_by_vuln.values():
            all_strategies.extend(strategies)
        
        # Step 3: Evaluate strategies
        strategies_by_vuln = self.evaluator.evaluate_strategies(strategies_by_vuln, vulnerabilities)
        
        # Step 4: Optimize selection
        budget = self.config.remediation_budget
        selected_strategies = self.optimizer.optimize(strategies_by_vuln, budget)
        
        # Calculate statistics
        total_value = sum(s.value_score for s in selected_strategies)
        total_cost = sum(s.cost_score for s in selected_strategies)
        budget_used = (total_cost / budget * 100) if budget > 0 else 0
        
        end_time = time.time()
        
        result = RemediationResult(
            vulnerabilities=vulnerabilities,
            all_strategies=all_strategies,
            selected_strategies=selected_strategies,
            total_value=total_value,
            total_cost=total_cost,
            budget_used=budget_used,
            optimization_time=end_time - start_time,
            statistics={
                "total_vulnerabilities": len(vulnerabilities),
                "total_strategies_generated": len(all_strategies),
                "strategies_selected": len(selected_strategies),
                "average_value_per_strategy": total_value / len(selected_strategies) if selected_strategies else 0,
                "average_cost_per_strategy": total_cost / len(selected_strategies) if selected_strategies else 0,
                "budget_efficiency": total_value / total_cost if total_cost > 0 else 0,
            }
        )
        
        logger.info(f"Remediation planning completed in {end_time - start_time:.2f}s. "
                   f"Selected {len(selected_strategies)} strategies with total value {total_value:.2f}")
        
        return result
