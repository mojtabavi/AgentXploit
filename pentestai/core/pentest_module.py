"""
Stage 1: Pentest Module - Autonomous Vulnerability Discovery

Implements the Pentest Module from the PenHeal paper with 6 agents:
1. Planner - Creates attack plans with counterfactual reasoning
2. Executor - Generates and executes commands  
3. Instructor - Provides RAG-based guidance from knowledge base
4. Summarizer - Condenses command outputs
5. Extractor - Parses vulnerabilities from results
6. Monitor - Tracks progress and handles errors (new)

Architecture based on: arXiv:2407.17788v1 (PenHeal Paper)
"""

import logging
import json
from typing import Any, Callable, Optional
from datetime import datetime

from pentestai.models.data import (
    AttackPlanNode,
    ExecutionResult,
    PentestPhase,
    PentestResult,
    TaskStatus,
    Vulnerability,
    VulnerabilitySeverity,
)
from pentestai.knowledge.base import search_knowledge
from pentestai.core.config import PentestAIConfig
from pentestai.llm.client import LLMClient, OpenAIClient, MockLLMClient
from pentestai.mcp.client import KaliMCPClient

logger = logging.getLogger(__name__)


class PlannerAgent:
    """
    Planner Agent - Creates and updates attack plans with counterfactual reasoning.
    
    Key features:
    - Hierarchical task structure (PTT-based)
    - Counterfactual prompting to explore multiple attack paths
    - Dynamic plan updates based on results
    - Real LLM integration for intelligent planning
    """

    def __init__(self, config: PentestAIConfig, llm_client: Optional[LLMClient] = None):
        self.config = config
        self.attack_plan: Optional[AttackPlanNode] = None
        self.vulnerabilities_found: list[str] = []
        
        # Initialize LLM client
        if llm_client:
            self.llm = llm_client
        elif config.openai_api_key:
            self.llm = OpenAIClient(
                api_key=config.openai_api_key,
                model=config.planner_model,
                base_url=config.openai_base_url,
                use_apikey_auth=config.use_apikey_auth,
            )
        else:
            logger.warning("No OpenAI API key - using mock LLM client")
            self.llm = MockLLMClient()

    def create_initial_plan(self, target: str, available_tools: Optional[list[str]] = None) -> AttackPlanNode:
        """
        Create initial attack plan following standard pentest phases.
        
        Args:
            target: Target system IP/hostname
            
        Returns:
            Root attack plan node
        """
        logger.info(f"Creating initial attack plan for target: {target}")
        
        # Create root node
        root = AttackPlanNode(
            id="0",
            phase=PentestPhase.RECONNAISSANCE,
            description="Complete Penetration Test",
            status=TaskStatus.IN_PROGRESS,
            metadata={"target": target}
        )
        
        # Phase 1: Reconnaissance
        recon = AttackPlanNode(
            id="1",
            phase=PentestPhase.RECONNAISSANCE,
            description="Reconnaissance - Information Gathering",
            status=TaskStatus.TODO,
            parent_id="0",
        )
        recon.subtasks = self._build_recon_tasks(target, available_tools)
        
        # Phase 2: Exploitation
        exploit = AttackPlanNode(
            id="2",
            phase=PentestPhase.EXPLOITATION,
            description="Exploitation - Attack vulnerable services",
            status=TaskStatus.TODO,
            parent_id="0",
        )
        
        root.subtasks = [recon, exploit]
        self.attack_plan = root
        
        logger.info("Initial attack plan created with reconnaissance and exploitation phases")
        return root

    def _build_recon_tasks(self, target: str, available_tools: Optional[list[str]] = None) -> list[AttackPlanNode]:
        """Build recon tasks using LLM-selected tools when possible."""
        if available_tools:
            llm_tasks = self._llm_recon_tasks(target, available_tools)
            if llm_tasks:
                return llm_tasks

        return [
            AttackPlanNode(
                id="1.1",
                phase=PentestPhase.RECONNAISSANCE,
                description=f"Port scan target {target}",
                status=TaskStatus.TODO,
                command=f"nmap -sV {target}",
                parent_id="1",
            ),
            AttackPlanNode(
                id="1.2",
                phase=PentestPhase.RECONNAISSANCE,
                description=f"Vulnerability scan target {target}",
                status=TaskStatus.TODO,
                command=f"nmap --script vuln {target}",
                parent_id="1",
            ),
        ]

    def _llm_recon_tasks(self, target: str, available_tools: list[str]) -> list[AttackPlanNode]:
        """Ask the LLM to pick tools and commands for recon."""
        system_prompt = """You are a penetration testing planner.
Select appropriate Kali tools for reconnaissance and return JSON only."""
        user_prompt = f"""
Target: {target}
Available tools: {', '.join(sorted(available_tools))}

Return JSON array of 2-4 tasks. Each item must be:
{{"description": "...", "command": "..."}}

Rules:
- Use only tools from the available list
- Commands must include the target
- Prefer safe recon tasks (port/service discovery, vuln scan)
"""
        try:
            response = self.llm.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.4,
                max_tokens=400,
            )
            content = response.content if hasattr(response, "content") else str(response)
            tasks_data = json.loads(content)
            if not isinstance(tasks_data, list):
                return []

            valid_tasks = []
            task_id = 1
            for item in tasks_data:
                command = item.get("command", "").strip()
                description = item.get("description", "").strip() or "Recon task"
                tool_name = command.split()[0] if command else ""
                if not command or tool_name not in available_tools:
                    continue
                if target not in command:
                    continue
                valid_tasks.append(
                    AttackPlanNode(
                        id=f"1.{task_id}",
                        phase=PentestPhase.RECONNAISSANCE,
                        description=description,
                        status=TaskStatus.TODO,
                        command=command,
                        parent_id="1",
                        metadata={"llm_selected": True},
                    )
                )
                task_id += 1

            return valid_tasks
        except Exception as e:
            logger.warning(f"LLM recon task selection failed: {e}")
            return []

    def apply_counterfactual_reasoning(self, vulnerabilities: list[str]) -> None:
        """
        Apply counterfactual prompting to explore alternative attack paths.
        
        This is a key innovation from the PenHeal paper: "Assume discovered
        vulnerabilities don't exist - what other vulnerabilities could be present?"
        
        Uses LLM to generate alternative exploration strategies.
        
        Args:
            vulnerabilities: List of already-discovered vulnerability names
        """
        logger.info(f"Applying counterfactual reasoning. Known vulnerabilities: {len(vulnerabilities)}")
        
        self.vulnerabilities_found.extend(vulnerabilities)
        
        if not vulnerabilities or not self.attack_plan:
            return
        
        # Construct counterfactual prompt for LLM
        system_prompt = """You are a penetration testing expert. Your task is to suggest 
        alternative attack vectors when asked to think counterfactually."""
        
        user_prompt = f"""
        We have discovered these vulnerabilities on target {self.config.target}:
        {', '.join(vulnerabilities)}
        
        Apply counterfactual reasoning: Assume these vulnerabilities do NOT exist.
        What other attack vectors, services, or misconfigurations should we explore?
        
        Suggest 2-3 specific reconnaissance or exploitation tasks in this format:
        1. Task description
        2. Specific command to execute
        
        Focus on commonly overlooked attack surfaces.
        """
        
        try:
            response = self.llm.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7,
                max_tokens=500,
            )
            
            # Parse response and add tasks to attack plan
            exploit_node = next((n for n in self.attack_plan.subtasks if n.id == "2"), None)
            if exploit_node:
                # Add counterfactual exploration task
                alt_task = AttackPlanNode(
                    id=f"2.{len(exploit_node.subtasks) + 1}",
                    phase=PentestPhase.EXPLOITATION,
                    description=f"Counterfactual exploration: {response.content[:100]}",
                    status=TaskStatus.TODO,
                    parent_id="2",
                    metadata={
                        "counterfactual": True,
                        "ignoring": vulnerabilities,
                        "llm_suggestion": response.content,
                    }
                )
                exploit_node.subtasks.append(alt_task)
                logger.info("Counterfactual reasoning applied - LLM generated alternative tasks")
        
        except Exception as e:
            logger.warning(f"LLM counterfactual reasoning failed: {e}. Using fallback logic.")
            # Fallback to simple logic
            exploit_node = next((n for n in self.attack_plan.subtasks if n.id == "2"), None)
            if exploit_node:
                alt_task = AttackPlanNode(
                    id=f"2.{len(exploit_node.subtasks) + 1}",
                    phase=PentestPhase.EXPLOITATION,
                    description="Explore alternative attack vectors (counterfactual)",
                    status=TaskStatus.TODO,
                    parent_id="2",
                    metadata={"counterfactual": True, "ignoring": vulnerabilities}
                )
                exploit_node.subtasks.append(alt_task)

    def update_task_status(self, task_id: str, status: TaskStatus, output_summary: str = "") -> None:
        """Update task status and output in the attack plan."""
        def _update_recursive(node: AttackPlanNode) -> bool:
            if node.id == task_id:
                node.status = status
                node.output_summary = output_summary
                return True
            for subtask in node.subtasks:
                if _update_recursive(subtask):
                    return True
            return False
        
        if self.attack_plan:
            if _update_recursive(self.attack_plan):
                logger.info(f"Updated task {task_id} status to {status}")
            else:
                logger.warning(f"Task {task_id} not found in attack plan")

    def get_next_task(self) -> Optional[AttackPlanNode]:
        """Get the next task to execute from the attack plan."""
        def _find_next(node: AttackPlanNode) -> Optional[AttackPlanNode]:
            if node.status == TaskStatus.TODO and node.command:
                return node
            for subtask in node.subtasks:
                result = _find_next(subtask)
                if result:
                    return result
            return None
        
        if self.attack_plan:
            return _find_next(self.attack_plan)
        return None


class ExecutorAgent:
    """
    Executor Agent - Generates and executes penetration testing commands.
    
    Responsibilities:
    - Generate commands based on tasks using LLM
    - Execute commands via Kali MCP server (when available)
    - Execute commands in simulated/sandboxed environment (fallback)
    - Format commands for parsing (wrapped in $...$)
    
    NEW: MCP Integration
    - Can communicate with Kali Linux MCP server
    - Discovers available tools dynamically
    - Executes real commands in isolated Kali container
    - Updates attack plan based on tool capabilities
    """

    def __init__(self, config: PentestAIConfig, llm_client: Optional[LLMClient] = None, mcp_client: Optional[KaliMCPClient] = None):
        self.config = config
        self.use_mcp = config.use_mcp if hasattr(config, 'use_mcp') else False
        self.mcp_client = mcp_client
        self.mcp_only = bool(getattr(config, "mcp_only", False))
        self.event_handler: Optional[Callable[[dict[str, Any]], None]] = getattr(config, "event_handler", None)
        logger.info(
            "ExecutorAgent init: use_mcp=%s, mcp_only=%s, kali_mcp_url=%s",
            self.use_mcp,
            self.mcp_only,
            self.config.kali_mcp_url,
        )
        
        # Initialize LLM client
        if llm_client:
            self.llm = llm_client
        elif config.openai_api_key:
            self.llm = OpenAIClient(
                api_key=config.openai_api_key,
                model=config.executor_model,
                base_url=config.openai_base_url,
                use_apikey_auth=config.use_apikey_auth,
            )
        else:
            logger.warning("No OpenAI API key - using mock LLM client")
            self.llm = MockLLMClient()
        
        # Initialize MCP client if enabled
        if self.use_mcp and not self.mcp_client:
            try:
                self.mcp_client = KaliMCPClient(base_url=self.config.kali_mcp_url)
                if self.mcp_client.health_check():
                    logger.info("âœ“ Connected to Kali MCP Server")
                    # List available tools
                    tools = self.mcp_client.list_tools()
                    logger.info(f"âœ“ {len(tools)} Kali tools available via MCP")
                else:
                    logger.warning("Kali MCP Server health check failed - falling back to simulation")
                    if self.mcp_only:
                        raise RuntimeError("MCP required but server health check failed")
                    self.use_mcp = False
                    self.mcp_client = None
            except Exception as e:
                logger.warning(f"Failed to connect to Kali MCP Server: {e} - falling back to simulation")
                if self.mcp_only:
                    raise
                self.use_mcp = False
                self.mcp_client = None

    def _emit_event(self, payload: dict[str, Any]) -> None:
        handler = self.event_handler
        if not handler:
            return
        try:
            handler(payload)
        except Exception as exc:
            logger.warning(f"Event handler failed: {exc}")

    def execute_task(self, task: AttackPlanNode, guidance: str = "") -> ExecutionResult:
        """
        Execute a penetration testing task.
        
        Args:
            task: Task from attack plan
            guidance: Optional guidance from Instructor
            
        Returns:
            Execution result with output/vulnerabilities
        """
        logger.info(f"Executing task {task.id}: {task.description}")
        
        command = task.command
        tool_name = command.split()[0] if command else "unknown"

        self._emit_event({
            "type": "tool_start",
            "task_id": task.id,
            "phase": task.phase.value,
            "tool": tool_name,
            "command": command,
            "description": task.description,
        })
        
        # Priority 1: Use MCP if available (real execution in Kali Linux)
        if self.use_mcp and self.mcp_client:
            output, success = self._execute_via_mcp(command, task)
        # Priority 2: Simulation mode
        elif self.config.sandbox_mode and not self.mcp_only:
            logger.warning("MCP unavailable, using simulated execution")
            output = self._simulate_execution(command, task.description)
            success = "ERROR" not in output and "FAILED" not in output
        # Priority 3: Real mode (disabled for safety)
        else:
            if self.mcp_only:
                raise RuntimeError("MCP-only execution is enabled but MCP is unavailable")
            logger.warning("Real execution mode - this should only be used in controlled sandboxes!")
            output = "REAL_EXECUTION_DISABLED_FOR_SAFETY"
            success = False
        
        # Parse for vulnerabilities
        vulnerabilities_found = self._parse_vulnerabilities(output)
        
        result = ExecutionResult(
            task_id=task.id,
            command=command,
            output=output,
            success=success,
            vulnerabilities_found=vulnerabilities_found,
            timestamp=datetime.now(),
        )

        trimmed_output = output[:4000] + "..." if len(output) > 4000 else output
        self._emit_event({
            "type": "tool_output",
            "task_id": task.id,
            "phase": task.phase.value,
            "tool": tool_name,
            "command": command,
            "success": success,
            "output": trimmed_output,
        })
        
        logger.info(f"Task {task.id} completed. Vulnerabilities found: {len(vulnerabilities_found)}")
        return result
    
    def _execute_via_mcp(self, command: str, task: AttackPlanNode) -> tuple[str, bool]:
        """
        Execute command via Kali MCP Server.
        
        Args:
            command: Command to execute
            task: Task node with metadata
            
        Returns:
            Tuple of (output, success)
        """
        try:
            # Extract tool name from command
            tool_name = command.split()[0]
            
            logger.info(f"ðŸ‰ Executing via Kali MCP: {tool_name}")
            
            # Get tool info for validation
            tool_info = self.mcp_client.get_tool_info(tool_name)
            if not tool_info:
                message = f"Tool '{tool_name}' not found in MCP"
                if self.mcp_only:
                    raise RuntimeError(message)
                logger.warning(f"{message} - falling back to simulation")
                return self._simulate_execution(command, task.description), True
            
            # Execute command
            result = self.mcp_client.execute_command(
                tool=tool_name,
                command=command,
                timeout=300  # 5 minutes default
            )
            
            # Combine stdout and stderr
            output = result.stdout
            if result.stderr:
                output += f"\n\nSTDERR:\n{result.stderr}"
            
            logger.info(f"âœ“ MCP execution completed: exit_code={result.exit_code}, time={result.execution_time}s")
            
            return output, result.success
            
        except Exception as e:
            logger.error(f"MCP execution failed: {e} - falling back to simulation")
            return self._simulate_execution(command, task.description), True
    
    def get_available_tools(self) -> list[str]:
        """
        Get list of available tools from MCP server.
        
        Returns:
            List of tool names
        """
        if self.use_mcp and self.mcp_client:
            tools = self.mcp_client.list_tools()
            return [tool.name for tool in tools]
        return []
    
    def get_tool_suggestions(self, task_description: str) -> list[dict]:
        """
        Get tool suggestions for a task from MCP server.
        
        Args:
            task_description: Description of the pentesting task
            
        Returns:
            List of suggested tools with info
        """
        if self.use_mcp and self.mcp_client:
            tools = self.mcp_client.get_tool_suggestions(task_description)
            return [
                {
                    "name": tool.name,
                    "description": tool.description,
                    "examples": tool.examples
                }
                for tool in tools
            ]
        return []

    def _simulate_execution(self, command: str, description: str) -> str:
        """
        Simulate command execution with symbolic outputs.
        
        This is SAFE for research - returns symbolic indicators instead of real attacks.
        """
        cmd_lower = command.lower()
        
        # Nmap port scan simulation
        if "nmap" in cmd_lower and "-sv" in cmd_lower:
            return """
            PORT_21_OPEN: FTP (vsFTPd 2.3.4)
            PORT_22_OPEN: SSH (OpenSSH 7.2)
            PORT_80_OPEN: HTTP (Apache 2.4.18)
            PORT_139_OPEN: NetBIOS (Samba 3.X)
            PORT_445_OPEN: SMB (Samba 3.X)
            PORT_6667_OPEN: IRC (UnrealIRCd 3.2.8.1)
            """
        
        # Nmap vulnerability scan simulation
        elif "nmap" in cmd_lower and "vuln" in cmd_lower:
            return """
            VULNERABILITY_DETECTED: CVE-2011-2523 (vsFTPd backdoor)
            VULNERABILITY_DETECTED: CVE-2010-2075 (UnrealIRCd backdoor)
            VULNERABILITY_DETECTED: CVE-2007-2447 (Samba usermap_script)
            SQL_INJECTION_POSSIBLE: Port 80
            """
        
        # Metasploit FTP backdoor simulation
        elif "vsftpd" in cmd_lower or "ftp" in cmd_lower:
            return """
            EXPLOIT_SUCCESSFUL: vsFTPd 2.3.4 backdoor triggered
            SHELL_OBTAINED: Root access established
            VULNERABILITY_CONFIRMED: CVE-2011-2523
            """
        
        # Metasploit IRC backdoor simulation
        elif "unreal" in cmd_lower or ("irc" in cmd_lower and "backdoor" in cmd_lower):
            return """
            EXPLOIT_SUCCESSFUL: UnrealIRCd backdoor triggered
            REVERSE_SHELL_ESTABLISHED: Command execution available
            VULNERABILITY_CONFIRMED: CVE-2010-2075
            """
        
        # Metasploit Samba exploitation simulation
        elif "samba" in cmd_lower or "usermap" in cmd_lower:
            return """
            EXPLOIT_SUCCESSFUL: Samba usermap_script RCE
            ROOT_SHELL_OBTAINED: Full system access
            VULNERABILITY_CONFIRMED: CVE-2007-2447
            """
        
        # SQLMap simulation
        elif "sqlmap" in cmd_lower:
            return """
            SQL_INJECTION_CONFIRMED: Parameter 'id' is vulnerable
            DATABASE_ACCESS_OBTAINED: MySQL database
            SENSITIVE_DATA_EXTRACTED: 5 tables dumped
            """
        
        # Default simulation
        else:
            return f"COMMAND_EXECUTED: {command}\nOUTPUT_SIMULATED: Task completed successfully"

    def _parse_vulnerabilities(self, output: str) -> list[str]:
        """Parse vulnerability indicators from command output."""
        vulnerabilities = []
        
        keywords = [
            "VULNERABILITY_DETECTED",
            "VULNERABILITY_CONFIRMED",
            "CVE-",
            "EXPLOIT_SUCCESSFUL",
            "SQL_INJECTION",
            "BACKDOOR",
        ]
        
        for line in output.split("\n"):
            if any(keyword in line for keyword in keywords):
                vulnerabilities.append(line.strip())
        
        return vulnerabilities


class InstructorAgent:
    """
    Instructor Agent - Provides RAG-based guidance from knowledge base.
    
    Uses the knowledge base to provide context-aware guidance for tasks.
    """

    def __init__(self, config: PentestAIConfig, llm_client: Optional[LLMClient] = None):
        self.config = config
        self.llm = llm_client or MockLLMClient()

    def get_guidance(self, task: AttackPlanNode) -> str:
        """
        Get guidance from knowledge base for a task.
        
        Args:
            task: Task needing guidance
            
        Returns:
            Formatted guidance text
        """
        if not self.config.use_instructor:
            return ""
        
        # Search knowledge base
        query = task.description + " " + task.command
        entries = search_knowledge(query, top_k=2)
        
        if not entries:
            return ""
        
        guidance_parts = ["Guidance from knowledge base:"]
        for i, entry in enumerate(entries, 1):
            guidance_parts.append(f"\n{i}. {entry.title}")
            guidance_parts.append(f"   {entry.content[:300]}...")
        
        return "\n".join(guidance_parts)


class SummarizerAgent:
    """
    Summarizer Agent - Condenses verbose command outputs.
    
    Addresses the context window limitation problem mentioned in the paper.
    """

    def __init__(self, config: PentestAIConfig, llm_client: Optional[LLMClient] = None):
        self.config = config
        self.llm = llm_client or MockLLMClient()

    def summarize(self, output: str, max_length: int = 500) -> str:
        """
        Summarize command output to essential information.
        
        Args:
            output: Raw command output
            max_length: Maximum summary length
            
        Returns:
            Summarized output
        """
        if len(output) <= max_length:
            return output
        
        # Extract key lines (those with uppercase keywords)
        important_lines = []
        for line in output.split("\n"):
            if any(keyword in line.upper() for keyword in [
                "PORT", "OPEN", "VULNERABILITY", "CVE", "EXPLOIT",
                "SUCCESS", "FAILED", "ERROR", "DETECTED", "FOUND"
            ]):
                important_lines.append(line)
        
        summary = "\n".join(important_lines)
        
        if len(summary) > max_length:
            summary = summary[:max_length] + "..."
        
        return summary if summary else output[:max_length] + "..."


class ExtractorAgent:
    """
    Extractor Agent - Parses structured vulnerability data from results.
    
    Converts symbolic/textual results into structured Vulnerability objects.
    """

    def __init__(self, config: PentestAIConfig, llm_client: Optional[LLMClient] = None):
        self.config = config
        self.llm = llm_client or MockLLMClient()

    def extract_vulnerabilities(self, execution_history: list[ExecutionResult]) -> list[Vulnerability]:
        """
        Extract structured vulnerabilities from execution history.
        
        Args:
            execution_history: List of command execution results
            
        Returns:
            List of structured Vulnerability objects
        """
        vulnerabilities = []
        
        for result in execution_history:
            for vuln_line in result.vulnerabilities_found:
                vuln = self._parse_vulnerability_line(vuln_line)
                if vuln:
                    vulnerabilities.append(vuln)
        
        # Deduplicate by CVE/name
        seen = set()
        unique_vulns = []
        for vuln in vulnerabilities:
            key = vuln.cve_id if vuln.cve_id else vuln.name
            if key not in seen:
                seen.add(key)
                unique_vulns.append(vuln)
        
        logger.info(f"Extracted {len(unique_vulns)} unique vulnerabilities from {len(execution_history)} executions")
        return unique_vulns

    def _parse_vulnerability_line(self, line: str) -> Optional[Vulnerability]:
        """Parse a single vulnerability indicator line."""
        # CVE-2011-2523 - vsFTPd backdoor
        if "CVE-2011-2523" in line or "vsftpd" in line.lower():
            return Vulnerability(
                name="vsFTPd 2.3.4 Backdoor",
                description="Backdoor in vsFTPd 2.3.4 allowing remote code execution",
                severity=VulnerabilitySeverity.CRITICAL,
                cvss_score=10.0,
                cve_id="CVE-2011-2523",
                service="FTP",
                port=21,
                exploit_method="Backdoor trigger",
            )
        
        # CVE-2010-2075 - UnrealIRCd backdoor
        elif "CVE-2010-2075" in line or "unrealircd" in line.lower():
            return Vulnerability(
                name="UnrealIRCd Backdoor",
                description="Trojaned UnrealIRCd with backdoor command execution",
                severity=VulnerabilitySeverity.HIGH,
                cvss_score=7.5,
                cve_id="CVE-2010-2075",
                service="IRC",
                port=6667,
                exploit_method="Backdoor command",
            )
        
        # CVE-2007-2447 - Samba usermap_script
        elif "CVE-2007-2447" in line or "samba" in line.lower():
            return Vulnerability(
                name="Samba usermap_script RCE",
                description="Command injection in Samba usermap_script",
                severity=VulnerabilitySeverity.MEDIUM,
                cvss_score=6.0,
                cve_id="CVE-2007-2447",
                service="SMB",
                port=445,
                exploit_method="Command injection",
            )
        
        # SQL Injection
        elif "SQL" in line.upper() and "INJECTION" in line.upper():
            return Vulnerability(
                name="SQL Injection",
                description="SQL injection vulnerability in web application",
                severity=VulnerabilitySeverity.HIGH,
                cvss_score=8.5,
                cve_id=None,
                service="HTTP",
                port=80,
                exploit_method="SQL injection",
            )
        
        return None


class PentestModule:
    """
    Complete Pentest Module (Stage 1) coordinating all agents.
    
    Workflow:
    1. Planner creates initial attack plan
    2. For each task:
       a. Instructor provides guidance
       b. Executor executes command
       c. Summarizer condenses output
       d. Planner updates plan
    3. After success: Apply counterfactual reasoning
    4. Extractor parses final vulnerabilities
    """

    def __init__(self, config: PentestAIConfig, llm_clients: Optional[dict] = None):
        self.config = config
        
        # Initialize agents with LLM clients
        if llm_clients:
            self.planner = PlannerAgent(config, llm_clients.get("planner"))
            self.executor = ExecutorAgent(config, llm_clients.get("executor"))
            self.instructor = InstructorAgent(config, llm_clients.get("utility"))
            self.summarizer = SummarizerAgent(config, llm_clients.get("utility"))
            self.extractor = ExtractorAgent(config, llm_clients.get("utility"))
        else:
            self.planner = PlannerAgent(config)
            self.executor = ExecutorAgent(config)
            self.instructor = InstructorAgent(config)
            self.summarizer = SummarizerAgent(config)
            self.extractor = ExtractorAgent(config)
        
        self.execution_history: list[ExecutionResult] = []
        self.counterfactual_round = 0

    def run_pentest(self, target: str) -> PentestResult:
        """
        Run complete penetration test on target.
        
        Args:
            target: Target system IP/hostname
            
        Returns:
            Complete pentest results
        """
        logger.info(f"Starting penetration test on target: {target}")
        start_time = datetime.now()
        
        # Create initial attack plan
        available_tools = self.executor.get_available_tools()
        attack_plan = self.planner.create_initial_plan(target, available_tools)
        
        # Execute tasks
        iteration = 0
        while iteration < self.config.max_iterations:
            iteration += 1
            
            # Get next task
            task = self.planner.get_next_task()
            if not task:
                logger.info("No more tasks to execute")
                break
            
            # Get guidance from Instructor
            guidance = self.instructor.get_guidance(task)
            
            # Execute task
            result = self.executor.execute_task(task, guidance)
            self.execution_history.append(result)
            
            # Summarize output
            summary = self.summarizer.summarize(result.output)
            result.summary = summary
            
            # Update task status
            status = TaskStatus.COMPLETED if result.success else TaskStatus.FAILED
            self.planner.update_task_status(task.id, status, summary)
            
            # Apply counterfactual reasoning if vulnerabilities found
            if result.vulnerabilities_found and self.counterfactual_round < self.config.counterfactual_rounds:
                self.counterfactual_round += 1
                self.planner.apply_counterfactual_reasoning(result.vulnerabilities_found)
                logger.info(f"Counterfactual reasoning round {self.counterfactual_round}")
        
        # Extract vulnerabilities
        vulnerabilities = self.extractor.extract_vulnerabilities(self.execution_history)
        
        # Create result
        end_time = datetime.now()
        result = PentestResult(
            target=target,
            start_time=start_time,
            end_time=end_time,
            vulnerabilities=vulnerabilities,
            attack_plan=attack_plan,
            execution_history=self.execution_history,
            statistics={
                "total_iterations": iteration,
                "tasks_executed": len(self.execution_history),
                "vulnerabilities_found": len(vulnerabilities),
                "counterfactual_rounds": self.counterfactual_round,
                "duration_seconds": (end_time - start_time).total_seconds(),
            }
        )
        
        logger.info(f"Penetration test completed. Found {len(vulnerabilities)} vulnerabilities in {iteration} iterations")
        return result
