"""
LLM Client for PentestAI

Provides abstraction for LLM communication with OpenAI API.
"""

import os
import json
from typing import Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None


@dataclass
class LLMResponse:
    """Response from LLM"""
    content: str
    model: str
    tokens_used: int
    finish_reason: str
    raw_response: Optional[Any] = None


class LLMClient(ABC):
    """Abstract base class for LLM clients"""
    
    @abstractmethod
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> LLMResponse:
        """Generate response from LLM"""
        pass


class OpenAIClient(LLMClient):
    """OpenAI API client for LLM communication"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4",
        base_url: Optional[str] = None,
    ):
        """
        Initialize OpenAI client
        
        Args:
            api_key: OpenAI API key (or from OPENAI_API_KEY env var)
            model: Model name (gpt-4, gpt-3.5-turbo, etc.)
            base_url: Optional custom API base URL
        """
        if not OPENAI_AVAILABLE:
            raise ImportError(
                "OpenAI package not installed. Install with: pip install openai"
            )
        
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "OpenAI API key required. Set OPENAI_API_KEY environment variable "
                "or pass api_key parameter."
            )
        
        self.model = model
        self.base_url = base_url
        
        # Initialize OpenAI client
        client_kwargs = {"api_key": self.api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
        
        self.client = OpenAI(**client_kwargs)
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> LLMResponse:
        """
        Generate response from OpenAI
        
        Args:
            prompt: User prompt
            system_prompt: Optional system prompt
            temperature: Sampling temperature (0.0-2.0)
            max_tokens: Maximum tokens in response
            
        Returns:
            LLMResponse object
        """
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            finish_reason = response.choices[0].finish_reason
            
            return LLMResponse(
                content=content,
                model=self.model,
                tokens_used=tokens_used,
                finish_reason=finish_reason,
                raw_response=response,
            )
            
        except Exception as e:
            raise RuntimeError(f"OpenAI API error: {str(e)}")
    
    def generate_json(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> dict:
        """
        Generate JSON response from OpenAI
        
        Args:
            prompt: User prompt (should request JSON output)
            system_prompt: Optional system prompt
            temperature: Sampling temperature
            max_tokens: Maximum tokens
            
        Returns:
            Parsed JSON dict
        """
        response = self.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        
        # Try to extract JSON from response
        content = response.content.strip()
        
        # Handle markdown code blocks
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        
        content = content.strip()
        
        try:
            return json.loads(content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to parse JSON from LLM response: {str(e)}\nResponse: {content}")
    
    def generate_with_functions(
        self,
        prompt: str,
        functions: list[dict],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> dict:
        """
        Generate response with function calling
        
        Args:
            prompt: User prompt
            functions: List of function definitions
            system_prompt: Optional system prompt
            temperature: Sampling temperature
            max_tokens: Maximum tokens
            
        Returns:
            Function call arguments as dict
        """
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                functions=functions,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            if response.choices[0].message.function_call:
                function_call = response.choices[0].message.function_call
                return {
                    "name": function_call.name,
                    "arguments": json.loads(function_call.arguments),
                }
            else:
                # No function call, return text response
                return {
                    "name": None,
                    "arguments": {},
                    "content": response.choices[0].message.content,
                }
                
        except Exception as e:
            raise RuntimeError(f"OpenAI API error with functions: {str(e)}")


class MockLLMClient(LLMClient):
    """Mock LLM client for testing (returns simulated responses)"""
    
    def __init__(self, model: str = "mock-gpt"):
        self.model = model
        self.call_count = 0
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> LLMResponse:
        """Generate mock response"""
        self.call_count += 1
        
        # Simple mock responses based on keywords
        if "vulnerability" in prompt.lower() or "scan" in prompt.lower():
            content = "PORT_21_OPEN | CVE-2011-2523 | VSFTPD_BACKDOOR"
        elif "remediation" in prompt.lower() or "fix" in prompt.lower():
            content = "Update vsftpd to version 3.0.3 or later"
        elif "plan" in prompt.lower():
            content = "1. Reconnaissance\n2. Scanning\n3. Exploitation"
        else:
            content = f"Mock response to: {prompt[:50]}..."
        
        return LLMResponse(
            content=content,
            model=self.model,
            tokens_used=len(content.split()),
            finish_reason="stop",
            raw_response=None,
        )
