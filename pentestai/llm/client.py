"""
LLM Client for PentestAI

Provides abstraction for LLM communication with OpenAI API.
Supports OpenAI and custom gateways like ArvanCloud AI Gateway.
"""

import os
import json
from typing import Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

try:
    from openai import OpenAI
    import httpx
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None
    httpx = None


@dataclass
class LLMResponse:
    """Response from LLM"""
    content: str
    model: str
    tokens_used: int
    finish_reason: str
    raw_response: Optional[Any] = None


class LLMClient(ABC):
    """Abstract base class for LLM clients"""
    
    @abstractmethod
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> LLMResponse:
        """Generate response from LLM"""
        pass


class OpenAIClient(LLMClient):
    """OpenAI API client for LLM communication (supports custom gateways)"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4",
        base_url: Optional[str] = None,
        use_apikey_auth: bool = False,
        token_in_url: bool = False,
    ):
        """
        Initialize OpenAI client
        
        Args:
            api_key: OpenAI API key (or from OPENAI_API_KEY env var)
            model: Model name (gpt-4, gpt-3.5-turbo, etc.)
            base_url: Optional custom API base URL (e.g., ArvanCloud)
            use_apikey_auth: Use "apikey" prefix instead of "Bearer" (for ArvanCloud)
            token_in_url: Token is embedded in URL, use dummy API key (for some ArvanCloud gateways)
        """
        if not OPENAI_AVAILABLE:
            raise ImportError(
                "OpenAI package not installed. Install with: pip install openai"
            )
        
        # If token is in URL, use the API key as is (not dummy)
        # Some gateways like ArvanCloud still validate the API key header
        if token_in_url:
            self.api_key = api_key or os.getenv("ARVANCLOUD_API_KEY") or "dummy-key-not-used"
        else:
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                raise ValueError(
                    "OpenAI API key required. Set OPENAI_API_KEY environment variable "
                    "or pass api_key parameter."
                )
        
        self.model = model
        self.base_url = base_url
        self.use_apikey_auth = use_apikey_auth
        self.token_in_url = token_in_url
        
        # Initialize OpenAI client
        client_kwargs = {"api_key": self.api_key}
        if base_url:
            client_kwargs["base_url"] = base_url
        
        # Don't use custom HTTP client for token-in-URL
        # The OpenAI client will still add auth headers which seems to work
        # For ArvanCloud gateways that use "apikey" prefix
        if use_apikey_auth and httpx and not token_in_url:
            # Create custom httpx client with proper authentication header
            http_client = httpx.Client(
                headers={
                    "Authorization": f"apikey {self.api_key}",
                    "Content-Type": "application/json",
                },
                timeout=60.0,
            )
            client_kwargs["http_client"] = http_client
        
        self.client = OpenAI(**client_kwargs)
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> LLMResponse:
        """
        Generate response from OpenAI
        
        Args:
            prompt: User prompt
            system_prompt: Optional system prompt
            temperature: Sampling temperature (0.0-2.0)
            max_tokens: Maximum tokens in response
            
        Returns:
            LLMResponse object
        """
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            finish_reason = response.choices[0].finish_reason
            
            return LLMResponse(
                content=content,
                model=self.model,
                tokens_used=tokens_used,
                finish_reason=finish_reason,
                raw_response=response,
            )
            
        except Exception as e:
            raise RuntimeError(f"OpenAI API error: {str(e)}")
    
    def generate_json(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> dict:
        """
        Generate JSON response from OpenAI
        
        Args:
            prompt: User prompt (should request JSON output)
            system_prompt: Optional system prompt
            temperature: Sampling temperature
            max_tokens: Maximum tokens
            
        Returns:
            Parsed JSON dict
        """
        response = self.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        
        # Try to extract JSON from response
        content = response.content.strip()
        
        # Handle markdown code blocks
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        
        content = content.strip()
        
        try:
            return json.loads(content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to parse JSON from LLM response: {str(e)}\nResponse: {content}")
    
    def generate_with_functions(
        self,
        prompt: str,
        functions: list[dict],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> dict:
        """
        Generate response with function calling
        
        Args:
            prompt: User prompt
            functions: List of function definitions
            system_prompt: Optional system prompt
            temperature: Sampling temperature
            max_tokens: Maximum tokens
            
        Returns:
            Function call arguments as dict
        """
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                functions=functions,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            if response.choices[0].message.function_call:
                function_call = response.choices[0].message.function_call
                return {
                    "name": function_call.name,
                    "arguments": json.loads(function_call.arguments),
                }
            else:
                # No function call, return text response
                return {
                    "name": None,
                    "arguments": {},
                    "content": response.choices[0].message.content,
                }
                
        except Exception as e:
            raise RuntimeError(f"OpenAI API error with functions: {str(e)}")


class MockLLMClient(LLMClient):
    """Mock LLM client for testing (returns simulated responses)"""
    
    def __init__(self, model: str = "mock-gpt"):
        self.model = model
        self.call_count = 0
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
    ) -> LLMResponse:
        """Generate mock response"""
        self.call_count += 1
        
        # Simple mock responses based on keywords
        if "vulnerability" in prompt.lower() or "scan" in prompt.lower():
            content = "PORT_21_OPEN | CVE-2011-2523 | VSFTPD_BACKDOOR"
        elif "remediation" in prompt.lower() or "fix" in prompt.lower():
            content = "Update vsftpd to version 3.0.3 or later"
        elif "plan" in prompt.lower():
            content = "1. Reconnaissance\n2. Scanning\n3. Exploitation"
        else:
            content = f"Mock response to: {prompt[:50]}..."
        
        return LLMResponse(
            content=content,
            model=self.model,
            tokens_used=len(content.split()),
            finish_reason="stop",
            raw_response=None,
        )
