"""
PentestAI Command-Line Interface

Provides three execution modes:
1. Full Assessment: Run both pentest and remediation
2. Pentest Only: Run only vulnerability discovery
3. Remediation Only: Run only remediation planning (requires vulnerability file)

Usage:
    # Full assessment
    python -m pentestai.cli --target 192.168.1.100 --mode full
    
    # Pentest only
    python -m pentestai.cli --target 192.168.1.100 --mode pentest
    
    # Remediation only
    python -m pentestai.cli --mode remediation --vulnerabilities vulns.json
    
    # With configuration file
    python -m pentestai.cli --config config.json
"""

import argparse
import json
import logging
import sys
from pathlib import Path
from typing import Optional

from pentestai.core.config import PentestAIConfig
from pentestai.core.controller import PentestAIController
from pentestai.models.data import Vulnerability

logger = logging.getLogger(__name__)


def print_banner():
    """Print PentestAI ASCII banner."""
    banner = """
    ╔═══════════════════════════════════════════════════════════════════╗
    ║                                                                   ║
    ║   ██████╗ ███████╗███╗   ██╗████████╗███████╗███████╗████████╗    ║
    ║   ██╔══██╗██╔════╝████╗  ██║╚══██╔══╝██╔════╝██╔════╝╚══██╔══╝    ║
    ║   ██████╔╝█████╗  ██╔██╗ ██║   ██║   █████╗  ███████╗   ██║       ║
    ║   ██╔═══╝ ██╔══╝  ██║╚██╗██║   ██║   ██╔══╝  ╚════██║   ██║       ║
    ║   ██║     ███████╗██║ ╚████║   ██║   ███████╗███████║   ██║       ║
    ║   ╚═╝     ╚══════╝╚═╝  ╚═══╝   ╚═╝   ╚══════╝╚══════╝   ╚═╝       ║
    ║                                                                   ║
    ║            ╔═══╗ ╔═══╗                                            ║
    ║            ║ A ║ ║ I ║    Automated Pentesting & Remediation      ║
    ║            ╚═══╝ ╚═══╝                                            ║
    ║                                                                   ║
    ║   A Two-Stage LLM Framework for Automated Penetration Testing     ║
    ║   and Optimal Remediation Planning                                ║
    ║                                                                   ║
    ║   Based on PenHeal Paper (arXiv:2407.17788v1)                     ║
    ║   FOR RESEARCH AND EDUCATIONAL PURPOSES ONLY                      ║
    ║                                                                   ║
    ╚═══════════════════════════════════════════════════════════════════╝
    """
    print(banner)


def setup_parser() -> argparse.ArgumentParser:
    """Setup command-line argument parser."""
    parser = argparse.ArgumentParser(
        description="PentestAI - Automated Penetration Testing and Remediation Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Full assessment
  python -m pentestai.cli --target 192.168.1.100 --mode full --sandbox
  
  # Pentest only with custom iterations
  python -m pentestai.cli --target 192.168.1.100 --mode pentest --max-iterations 30
  
  # Remediation only from vulnerability file
  python -m pentestai.cli --mode remediation --vulnerabilities vulns.json --budget 50
  
  # Using configuration file
  python -m pentestai.cli --config config.json
  
  # With environment variables
  export PENTESTAI_TARGET="192.168.1.100"
  python -m pentestai.cli --mode full

For more information, see: https://github.com/your-repo/pentestai
        """
    )
    
    # Mode selection
    parser.add_argument(
        "--mode",
        choices=["full", "pentest", "remediation"],
        default="full",
        help="Execution mode: full (pentest+remediation), pentest (stage 1 only), remediation (stage 2 only)"
    )
    
    # Target configuration
    parser.add_argument(
        "--target",
        type=str,
        help="Target system IP address or hostname (required for pentest/full modes)"
    )
    
    parser.add_argument(
        "--config",
        type=str,
        help="Path to configuration file (JSON/YAML)"
    )
    
    # Pentest Module options
    pentest_group = parser.add_argument_group("Pentest Module Options")
    pentest_group.add_argument(
        "--max-iterations",
        type=int,
        default=50,
        help="Maximum pentest iterations (default: 50)"
    )
    pentest_group.add_argument(
        "--counterfactual-rounds",
        type=int,
        default=3,
        help="Number of counterfactual reasoning rounds (default: 3)"
    )
    pentest_group.add_argument(
        "--no-instructor",
        action="store_true",
        help="Disable RAG-based Instructor agent"
    )
    
    # Remediation Module options
    remediation_group = parser.add_argument_group("Remediation Module Options")
    remediation_group.add_argument(
        "--budget",
        type=float,
        default=100.0,
        help="Total remediation budget (default: 100.0)"
    )
    remediation_group.add_argument(
        "--budget-per-vuln",
        type=float,
        default=4.0,
        help="Default budget per vulnerability (default: 4.0)"
    )
    remediation_group.add_argument(
        "--no-optimization",
        action="store_true",
        help="Disable Group Knapsack optimization (use greedy selection)"
    )
    remediation_group.add_argument(
        "--vulnerabilities",
        type=str,
        help="Path to vulnerability JSON file (required for remediation-only mode)"
    )
    
    # LLM options
    llm_group = parser.add_argument_group("LLM Options")
    llm_group.add_argument(
        "--planner-model",
        type=str,
        default="gpt-4",
        help="Model for Planner agent (default: gpt-4)"
    )
    llm_group.add_argument(
        "--executor-model",
        type=str,
        default="gpt-4",
        help="Model for Executor agent (default: gpt-4)"
    )
    llm_group.add_argument(
        "--advisor-model",
        type=str,
        default="gpt-4",
        help="Model for Advisor agent (default: gpt-4)"
    )
    llm_group.add_argument(
        "--evaluator-model",
        type=str,
        default="gpt-4",
        help="Model for Evaluator agent (default: gpt-4)"
    )
    llm_group.add_argument(
        "--utility-model",
        type=str,
        default="gpt-3.5-turbo",
        help="Model for utility agents (default: gpt-3.5-turbo)"
    )
    llm_group.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="LLM temperature (default: 0.7)"
    )
    llm_group.add_argument(
        "--api-key",
        type=str,
        help="OpenAI API key (or set OPENAI_API_KEY environment variable)"
    )
    
    # Output options
    output_group = parser.add_argument_group("Output Options")
    output_group.add_argument(
        "--output-dir",
        type=str,
        default="./pentestai_results",
        help="Output directory for results (default: ./pentestai_results)"
    )
    output_group.add_argument(
        "--no-report",
        action="store_true",
        help="Disable human-readable report generation"
    )
    output_group.add_argument(
        "--no-attack-plan",
        action="store_true",
        help="Disable attack plan tree saving"
    )
    output_group.add_argument(
        "--export-format",
        choices=["json", "csv", "html"],
        default="json",
        help="Export format (default: json)"
    )
    
    # Safety options
    safety_group = parser.add_argument_group("Safety Options")
    safety_group.add_argument(
        "--sandbox",
        action="store_true",
        default=True,
        help="Run in sandbox mode (simulated commands) - RECOMMENDED"
    )
    safety_group.add_argument(
        "--no-sandbox",
        action="store_true",
        help="Disable sandbox mode - DANGEROUS! Only use in controlled environments"
    )
    safety_group.add_argument(
        "--require-confirmation",
        action="store_true",
        help="Require user confirmation for risky operations"
    )
    
    # Verbosity
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Enable verbose output"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )
    
    return parser


def load_config_from_args(args: argparse.Namespace) -> PentestAIConfig:
    """
    Load configuration from command-line arguments.
    
    Args:
        args: Parsed command-line arguments
        
    Returns:
        PentestAIConfig instance
    """
    # Load from file if specified
    if args.config:
        config = PentestAIConfig.from_file(args.config)
        logger.info(f"Loaded configuration from {args.config}")
    else:
        config = PentestAIConfig()
    
    # Override with command-line arguments
    if args.target:
        config.target = args.target
    
    config.max_iterations = args.max_iterations
    config.counterfactual_rounds = args.counterfactual_rounds
    config.use_instructor = not args.no_instructor
    
    config.remediation_budget = args.budget
    config.budget_per_vulnerability = args.budget_per_vuln
    config.enable_optimization = not args.no_optimization
    
    config.planner_model = args.planner_model
    config.executor_model = args.executor_model
    config.advisor_model = args.advisor_model
    config.evaluator_model = args.evaluator_model
    config.utility_model = args.utility_model
    config.temperature = args.temperature
    
    if args.api_key:
        config.api_key = args.api_key
    
    config.output_directory = args.output_dir
    config.generate_report = not args.no_report
    config.save_attack_plan = not args.no_attack_plan
    config.export_format = args.export_format
    
    if args.no_sandbox:
        config.sandbox_mode = False
        logger.warning("⚠️  SANDBOX MODE DISABLED - Real commands will be executed!")
    else:
        config.sandbox_mode = True
    
    config.require_confirmation = args.require_confirmation
    
    return config


def load_vulnerabilities_from_json(file_path: str) -> list[Vulnerability]:
    """Load vulnerabilities from JSON file."""
    with open(file_path, "r") as f:
        data = json.load(f)
    
    vulnerabilities = []
    
    # Handle different JSON formats
    if isinstance(data, list):
        vuln_dicts = data
    elif isinstance(data, dict) and "vulnerabilities" in data:
        vuln_dicts = data["vulnerabilities"]
    else:
        raise ValueError("Invalid vulnerability JSON format")
    
    for vuln_dict in vuln_dicts:
        # Reconstruct Vulnerability object
        vuln = Vulnerability(
            id=vuln_dict.get("id", ""),
            name=vuln_dict.get("name", ""),
            description=vuln_dict.get("description", ""),
            cvss_score=vuln_dict.get("cvss_score", 0.0),
            cve_id=vuln_dict.get("cve_id"),
            service=vuln_dict.get("service", ""),
            port=vuln_dict.get("port"),
            exploit_method=vuln_dict.get("exploit_method", ""),
        )
        vulnerabilities.append(vuln)
    
    logger.info(f"Loaded {len(vulnerabilities)} vulnerabilities from {file_path}")
    return vulnerabilities


def run_full_mode(args: argparse.Namespace) -> int:
    """Run full assessment mode."""
    config = load_config_from_args(args)
    
    if not config.target:
        logger.error("--target is required for full assessment mode")
        return 1
    
    controller = PentestAIController(config)
    
    try:
        pentest_result, remediation_result = controller.run_full_assessment()
        
        # Print summary
        print("\n" + "="*80)
        print("ASSESSMENT SUMMARY")
        print("="*80)
        print(f"Vulnerabilities Found: {len(pentest_result.vulnerabilities)}")
        print(f"Remediation Strategies Selected: {len(remediation_result.selected_strategies)}")
        print(f"Total Value: {remediation_result.total_value:.2f}")
        print(f"Total Cost: {remediation_result.total_cost:.2f}")
        print(f"Budget Used: {remediation_result.budget_used:.1f}%")
        print("="*80)
        
        return 0
    
    except Exception as e:
        logger.error(f"Assessment failed: {e}", exc_info=args.debug)
        return 1


def run_pentest_mode(args: argparse.Namespace) -> int:
    """Run pentest-only mode."""
    config = load_config_from_args(args)
    
    if not config.target:
        logger.error("--target is required for pentest mode")
        return 1
    
    controller = PentestAIController(config)
    
    try:
        result = controller.run_pentest_only()
        
        # Print summary
        print("\n" + "="*80)
        print("PENTEST SUMMARY")
        print("="*80)
        print(f"Vulnerabilities Found: {len(result.vulnerabilities)}")
        print(f"Tasks Executed: {result.statistics['tasks_executed']}")
        print(f"Duration: {result.statistics['duration_seconds']:.2f}s")
        print("="*80)
        
        return 0
    
    except Exception as e:
        logger.error(f"Pentest failed: {e}", exc_info=args.debug)
        return 1


def run_remediation_mode(args: argparse.Namespace) -> int:
    """Run remediation-only mode."""
    config = load_config_from_args(args)
    
    if not args.vulnerabilities:
        logger.error("--vulnerabilities is required for remediation mode")
        return 1
    
    # Load vulnerabilities
    try:
        vulnerabilities = load_vulnerabilities_from_json(args.vulnerabilities)
    except Exception as e:
        logger.error(f"Failed to load vulnerabilities: {e}")
        return 1
    
    controller = PentestAIController(config)
    
    try:
        result = controller.run_remediation_only(vulnerabilities)
        
        # Print summary
        print("\n" + "="*80)
        print("REMEDIATION SUMMARY")
        print("="*80)
        print(f"Vulnerabilities: {len(vulnerabilities)}")
        print(f"Strategies Generated: {len(result.all_strategies)}")
        print(f"Strategies Selected: {len(result.selected_strategies)}")
        print(f"Total Value: {result.total_value:.2f}")
        print(f"Total Cost: {result.total_cost:.2f}")
        print(f"Budget Used: {result.budget_used:.1f}%")
        print("="*80)
        
        return 0
    
    except Exception as e:
        logger.error(f"Remediation failed: {e}", exc_info=args.debug)
        return 1


def main() -> int:
    """Main CLI entry point."""
    parser = setup_parser()
    args = parser.parse_args()
    
    # Setup logging
    log_level = logging.DEBUG if args.debug else (logging.INFO if args.verbose else logging.WARNING)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Print banner
    print_banner()
    
    # Safety warning for non-sandbox mode
    if args.no_sandbox:
        print("\n" + "⚠️ "*40)
        print("WARNING: SANDBOX MODE IS DISABLED!")
        print("Real penetration testing commands will be executed.")
        print("This should ONLY be used in controlled, isolated environments.")
        print("Press Ctrl+C to cancel within 5 seconds...")
        print("⚠️ "*40 + "\n")
        
        try:
            import time
            time.sleep(5)
        except KeyboardInterrupt:
            print("\n\nCancelled by user.")
            return 1
    
    # Dispatch to appropriate mode
    if args.mode == "full":
        return run_full_mode(args)
    elif args.mode == "pentest":
        return run_pentest_mode(args)
    elif args.mode == "remediation":
        return run_remediation_mode(args)
    else:
        parser.print_help()
        return 1


if __name__ == "__main__":
    sys.exit(main())
